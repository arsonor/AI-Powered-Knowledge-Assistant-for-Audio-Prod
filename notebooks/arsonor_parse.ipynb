{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts articles url, title and category from the sitemap page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles_from_sitemap(sitemap_url):\n",
    "    response = requests.get(sitemap_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the sitemap. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # List to store the extracted articles\n",
    "    articles = []\n",
    "\n",
    "    # Find all categories (strong with class 'wsp-category-title') and their corresponding articles\n",
    "    for category_section in soup.find_all('ul', class_='wsp-posts-list'):\n",
    "        category_title_tag = category_section.find_previous('strong', class_='wsp-category-title')\n",
    "        \n",
    "        if category_title_tag:\n",
    "            category_link = category_title_tag.find('a')\n",
    "            if category_link:\n",
    "                category_name = category_link.get_text(strip=True)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Now find the articles under this category\n",
    "        article_items = category_section.find_all('li', class_='wsp-post')\n",
    "        \n",
    "        for article_item in article_items:\n",
    "            article_link = article_item.find('a')\n",
    "            if article_link:\n",
    "                article_url = article_link['href']\n",
    "                article_title = article_link.get_text(strip=True)\n",
    "                \n",
    "                # Add the article details to the list\n",
    "                articles.append({\n",
    "                    'category': category_name,\n",
    "                    'title': article_title,\n",
    "                    'url': article_url\n",
    "                })\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetches and cleans the content of an article from the given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_info(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the article. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the main content div\n",
    "    content_div = soup.find('div', class_='entry-content')\n",
    "    # If the content div is not found, return None\n",
    "    if not content_div:\n",
    "        print(\"No content found with the class 'entry-content'.\")\n",
    "        return None\n",
    "\n",
    "    # Remove <div id=\"ez-toc-container\"...>\n",
    "    toc_container = content_div.find('div', id='ez-toc-container')\n",
    "    if toc_container:\n",
    "        toc_container.decompose()\n",
    "\n",
    "    # Remove all <div> elements where the class contains \"heateor\"\n",
    "    for heateor_div in content_div.find_all('div', class_=lambda x: x and 'heateor' in x):\n",
    "        heateor_div.decompose()\n",
    "\n",
    "    # Remove the specific <div class=\"elementor-column elementor-col-33...\"> within the <div class=\"elementor-container elementor-column-gap-default\">\n",
    "    for container_div in content_div.find_all('div', class_=lambda x: x and 'elementor-column elementor-col-33' in x):\n",
    "        container_div.decompose()\n",
    "\n",
    "    # Extract the cleaned text content\n",
    "    content = content_div.get_text(separator=\"\\n\").strip()\n",
    "    content = re.sub(r'\\n+', ' ', content)\n",
    "    content = content.replace('\\t', '')\n",
    "    content = content.replace(u'\\xa0', ' ')\n",
    "    content = content.strip()\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    \n",
    "    cleaned_content = content\n",
    "    \n",
    "    # Find the footer where the tags are located\n",
    "    footer = soup.find('footer', class_='entry-footer')\n",
    "    tags = ''\n",
    "    if footer:\n",
    "        # Extract tags from the second 'span' inside 'footer'\n",
    "        tags_span = footer.find_all('span', class_='entry-meta')\n",
    "        if len(tags_span) > 1:\n",
    "            # Second 'span' should contain the tags\n",
    "            tags_links = tags_span[1].find_all('a', rel='tag')\n",
    "            tags = ', '.join(tag.get_text() for tag in tags_links)\n",
    "\n",
    "    # Return article info as a dictionary\n",
    "    article_info = {\n",
    "        'text': cleaned_content,\n",
    "        'tags': tags\n",
    "    }\n",
    "\n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'écoute critique, espace sonore, mono, pan, profondeur, psycho-acoustique, reverb, stéréo'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_article_info(\"https://arsonor.com/comment-eduquer-loreille-a-lart-du-mixage-part-3/\")['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divides text in different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=240, overlap_size=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(words)\n",
    "\n",
    "    while start < text_length:\n",
    "        # Determine the end index for the chunk\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Append the chunk to the list\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move the start index forward by chunk_size - overlap_size\n",
    "        start += chunk_size - overlap_size\n",
    "        \n",
    "        # If the end is beyond the text length, break the loop\n",
    "        if end >= text_length:\n",
    "            break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates the json file with the addition of a unique Id for each article and chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_from_sitemap(sitemap_url, output_file, chunk_size=240, overlap_size=20):\n",
    " \n",
    "    articles = extract_articles_from_sitemap(sitemap_url)\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles found.\")\n",
    "        return\n",
    "\n",
    "    all_chunks_data = []\n",
    "\n",
    "    for article in tqdm(articles):\n",
    "        article_content = extract_article_info(article['url'])\n",
    "\n",
    "        if article_content:\n",
    "            combined = f\"{article['title']}-{article_content['text'][:10]}\"\n",
    "            hash_object = hashlib.md5(combined.encode())\n",
    "            hash_hex = hash_object.hexdigest()\n",
    "            article_id = hash_hex[:8]\n",
    "\n",
    "            chunks = chunk_text(article_content['text'], chunk_size=chunk_size, overlap_size=overlap_size)\n",
    "\n",
    "            # Create a chunk entry for each chunk of the article\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_id = f\"{article_id}-{i+1}\"  # e.g., article_id-1, article_id-2, etc.\n",
    "                all_chunks_data.append({\n",
    "                    'article_id': article_id,\n",
    "                    'title': article['title'],\n",
    "                    'category': article['category'],\n",
    "                    'tags': article_content['tags'],\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'chunk_text': chunk\n",
    "                })\n",
    "            \n",
    "        else:\n",
    "            print(f\"Skipping article: {article['title']} due to missing content.\")\n",
    "    \n",
    "    # Save the result to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_chunks_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b63ee3bd124e3eafeb561ed510b8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to arsonor_chunks_id.json\n"
     ]
    }
   ],
   "source": [
    "sitemap_url = 'https://arsonor.com/plan-du-site/'\n",
    "output_file = 'arsonor_chunks_id.json'\n",
    "\n",
    "create_json_from_sitemap(sitemap_url, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arsonor_chunks_id.json', 'r', encoding='utf-8') as file:\n",
    "    documents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(documents, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id unicity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 589)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hashes = defaultdict(list)\n",
    "for doc in documents:\n",
    "    doc_id = doc['chunk_id']\n",
    "    hashes[doc_id].append(doc)\n",
    "len(hashes), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4615db39</td>\n",
       "      <td>L’intelligence artificielle (IA) dans le studi...</td>\n",
       "      <td>LA POST-PROD</td>\n",
       "      <td>collaboration IA, intelligence artificielle, p...</td>\n",
       "      <td>4615db39-1</td>\n",
       "      <td>Suite et fin du tour d’horizon des logiciels e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4615db39</td>\n",
       "      <td>L’intelligence artificielle (IA) dans le studi...</td>\n",
       "      <td>LA POST-PROD</td>\n",
       "      <td>collaboration IA, intelligence artificielle, p...</td>\n",
       "      <td>4615db39-2</td>\n",
       "      <td>ce cas, les prochaines évolutions IA sont de v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4615db39</td>\n",
       "      <td>L’intelligence artificielle (IA) dans le studi...</td>\n",
       "      <td>LA POST-PROD</td>\n",
       "      <td>collaboration IA, intelligence artificielle, p...</td>\n",
       "      <td>4615db39-3</td>\n",
       "      <td>ans! Il n’y a pas d’autre choix que d’ embrass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4615db39</td>\n",
       "      <td>L’intelligence artificielle (IA) dans le studi...</td>\n",
       "      <td>LA POST-PROD</td>\n",
       "      <td>collaboration IA, intelligence artificielle, p...</td>\n",
       "      <td>4615db39-4</td>\n",
       "      <td>outils utilisant l’intelligence artificielle s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4615db39</td>\n",
       "      <td>L’intelligence artificielle (IA) dans le studi...</td>\n",
       "      <td>LA POST-PROD</td>\n",
       "      <td>collaboration IA, intelligence artificielle, p...</td>\n",
       "      <td>4615db39-5</td>\n",
       "      <td>les outils IA de traitement du langage naturel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title      category  \\\n",
       "0   4615db39  L’intelligence artificielle (IA) dans le studi...  LA POST-PROD   \n",
       "1   4615db39  L’intelligence artificielle (IA) dans le studi...  LA POST-PROD   \n",
       "2   4615db39  L’intelligence artificielle (IA) dans le studi...  LA POST-PROD   \n",
       "3   4615db39  L’intelligence artificielle (IA) dans le studi...  LA POST-PROD   \n",
       "4   4615db39  L’intelligence artificielle (IA) dans le studi...  LA POST-PROD   \n",
       "\n",
       "                                                tags    chunk_id  \\\n",
       "0  collaboration IA, intelligence artificielle, p...  4615db39-1   \n",
       "1  collaboration IA, intelligence artificielle, p...  4615db39-2   \n",
       "2  collaboration IA, intelligence artificielle, p...  4615db39-3   \n",
       "3  collaboration IA, intelligence artificielle, p...  4615db39-4   \n",
       "4  collaboration IA, intelligence artificielle, p...  4615db39-5   \n",
       "\n",
       "                                          chunk_text  \n",
       "0  Suite et fin du tour d’horizon des logiciels e...  \n",
       "1  ce cas, les prochaines évolutions IA sont de v...  \n",
       "2  ans! Il n’y a pas d’autre choix que d’ embrass...  \n",
       "3  outils utilisant l’intelligence artificielle s...  \n",
       "4  les outils IA de traitement du langage naturel...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_file_path = '../data/arsonor_chunks_id.json'\n",
    "df = pd.read_json(json_file_path)\n",
    "df[df['article_id'] == '4615db39']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
