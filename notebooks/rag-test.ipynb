{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbd7bbe-145f-4e58-a874-b005dbed225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import minsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454e399-424c-422e-9c60-8a83cf54b729",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c60918-87b5-4156-88b7-291eb26ed85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arsonor_data.json', 'r', encoding='utf-8') as file:\n",
    "    documents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d3d15f-2c46-47ed-a0ac-1df6d3f56159",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['title', 'text', 'tags'],\n",
    "    keyword_fields=['category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376cc4c0-efd8-4437-ba79-2de0a7a02265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x28caf0ad7f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4c969-765d-4ad2-b773-30cd5a790bf9",
   "metadata": {},
   "source": [
    "# RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1079a01c-d246-4f30-9912-8b97aefdc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62f684d0-f0d5-46fd-8d0a-5e5fbfe7c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dd6789d-dbcd-4bb8-a19c-70213e286f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How to create a good, punchy beat for my productions?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73c08c8-ce03-43ec-ad67-f9cad23e0d23",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1025\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1075\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1025\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1075\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\AI-Powered-Knowledge-Assistant-for-Audio-P-kOHx9R9c\\Lib\\site-packages\\openai\\_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[0;32m   1050\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": query}]\n",
    "    )\n",
    "    \n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "297ea691-22dd-4982-abd3-4104f6467a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "460e2c4b-d2b5-416a-aeb8-b2a503f8c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're an audio engineer and sound designer instructor for beginners.\n",
    "You're particularly specialized in audio home-studio set-up, computer music production and audio post-production in general (editing, mixing and mastering). \n",
    "Answer the QUESTION based on the CONTEXT from our arsonor knowledge database (articles).\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "Finally add in your response the top 5 articles of arsonor that are the best to read for answering this question.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "entry_template = \"\"\"\n",
    "article_title: {title}\n",
    "article_content: {text}\n",
    "article_keywords: {tags}\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6383065f-7541-4718-8c05-b577ade7d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(query)\n",
    "prompt = build_prompt(query, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a90e3571-822d-457d-815a-38530f98c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're an audio engineer and sound designer instructor for beginners.\n",
      "You're particularly specialized in audio home-studio set-up, computer music production and audio post-production in general (editing, mixing and mastering). \n",
      "Answer the QUESTION based on the CONTEXT from our arsonor knowledge database (articles).\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "Finally add in your response the top 5 articles of arsonor that are the best to read for answering this question.\n",
      "\n",
      "QUESTION: How to create a good, punchy beat for my productions?\n",
      "\n",
      "CONTEXT:\n",
      "article_title: Ecouter les sons du quotidien pour améliorer vos productions\n",
      "article_content: Voici le premier épisode du podcast Arsonor! Je t ‘y explique notamment: l’importance de savoir écouter les subtilités du son pour améliorer tes compétences en sound design et mixage audio. en quoi consiste l’écoute analytique/active et les bonnes questions qu’il faut se poser quand on écoute une musique. la relation entre ces questions et le processing audio pour retranscrire au mieux nos intentions de production, le tout avec un exemple concret. Pour écouter cet épisode, lance tout simplement le lecteur ci-dessus. Tu peux aussi accéder à la page officielle du podcast . Et bien sûr le télécharger en qualité audio optimale par le lien suivant . Contenu de l'épisode #1: Pour ceux qui tomberaient ici par hasard, ce blog parle de création et de production musicale, de A à Z depuis son home-studio, et plus particulièrement « in the box », c’est-à-dire tous les processing directement dans son ordinateur. C’est un gros programme tout ça, qui exige beaucoup de savoir-faire, d’expérience engrangée jour après jour, voire année après année. On en apprend tous les jours! Il faut se mettre à la fois dans la peau d’un compositeur et d’un ingénieur du son . C’est ce que j’appelle le producteur moderne , qui fait tout depuis chez lui. De l’idée du morceau jusqu’à sa diffusion. Alors, l’une des clés de l’apprentissage, ça va être d’ apprendre à écouter les sons . Cela a l’air trivial dis comme ça, mais en fait, quand par exemple tu écoutes de la musique, tu ne fais pas que ça. Tu ne l’écoutes pas forcément de manière très attentive, à faire attention jusqu’aux moindres détails. Mais si tu veux devenir producteur musical, l’un des savoir-faire les plus importants, c’est de savoir écouter les sons, se concentrer uniquement sur le son, pour ce qu’il est. Tu vas me dire c’est bien joli tout ça mais ça veut dire quoi? Comment je dois écouter? Et bien c’est justement le fil rouge de cet épisode et de tous ceux à venir , l’écoute analytique (active) des sons dans ce qui font leur forme pour mieux savoir les reproduire avec les technologies numériques actuelles. Et quoi de mieux que le format podcast pour se concentrer uniquement sur l’écoute et devenir ainsi un meilleur producteur? D’où le Nom de ce podcast: Ecouter les sons pour vos productions . L'importance de l'écoute analytique en production musicale Par analogie avec le film par exemple, un étudiant en cinéma qui regarde un film va analyser tous ses aspects techniques, faire attention aux mouvements de la caméra, l’éclairage, le type de plan, le découpage, la synchronisation des lèvres des acteurs, etc… Bien qu’il soit beaucoup moins agréable d’analyser les aspects techniques d’un film en le regardant, cela peut faire des étudiants en cinéma de bien meilleurs cinéastes. Et bien en musique, en production musicale, c’est exactement pareil! L’apprentissage consiste simplement à appuyer sur lecture et à écouter activement ce qui se passe et cela fera de toi un bien meilleur producteur musical. Chaque morceau, qu’il soit bon ou mauvais, est une leçon de production et de mixage . Il y a toujours quelque chose à en retirer. Alors assis-toi, ferme les yeux et écoute attentivement ce que les pros ont réalisé, simple non? OK, non en fait c’est pas si simple que ça 🙂 Comment écouter activement la musique ? Lors de l’écoute, il est bon de prendre l’habitude de se poser un certain nombres de question dans quatre catégories de la création musicale: La composition L’arrangement et l’instrumentation La production et le sound design Le mixage Ces questions permettent de se focaliser sur un aspect spécifique dans le processus de création musicale. 1) Composition Quelle est la tonalité du morceau? Est-ce que je peux remarquer certains intervalles entre les notes? Y’a t’il une progression d’accord particulière? Quelle est la signature rythmique du morceau? Est-ce que je peux remarquer un certain groove (légers décalages de notes)? etc… Par ces questions, je fais ici intervenir mon oreille musicale, celle du musicien. 2) Arrangement et instrumentation Combien d’instruments sont présents? Quels sont-ils? Quelle est la structure du morceau, si tenté qu’il y’en ait une? Qu’est-ce qui différencie le couplet du refrain? Peut-on remarquer d’autres sections (Break, etc…) et que s’y passe t’il? 3) Production/sound design C’est maintenant l’oreille de l’ingénieur du son que je vais solliciter pour me demander ce qui fait le timbre du son ? Quelle est l’évolution du son d’un élément en particulier au cours du temps au niveau de son enveloppe ? Est-il soutenu ou instantané/percussif avec beaucoup de transitoires? Observe-t’on une variation cyclique en volume, en pitch ou dans l’espace stéréo? Y’a t’il des effets de distorsions? des filtrages particuliers? des delay, reverb, effet autotune, etc… 4) Mixage Cette fois, il s’agit de prêter attention au comportement des divers instruments/éléments entre eux, à la fois sur le plan dynamique, fréquentiel, et de gestion de l’espace: Quelle est le niveau des instruments les uns par rapport aux autres? Comment les instruments sont-ils pannés (positionnés dans l’espace stéréo)? Comment les différents instruments sont-ils disposés sur le spectre des fréquences (des basses au plus hautes)? Dans quelle mesure les instruments sont-ils reliés les uns aux autres? Y a-t-il une automation? sur quel paramètre en particulier? Quelle est la durée des réverbérations? Quelle est la définition des instruments? Ce ne sont que quelques exemple d’interrogations parmi les innombrables choses à rechercher lors de l’analyse de mixages. Première difficulté de l'apprenti producteur musical Quand on débute en production musicale, il est en effet difficile de faire le lien entre nos interrogations et les processeurs et outils de la production. Comment alors répondre à toutes ces questions? Par exemple, des questionnements comme « je veux faire avancer ce son vers moi », « je n’entends pas cet élément par dessus l’autre », « je veux cet instrument plus sombre et plus diffus », etc…, n’ont pas de relation immédiate avec des termes comme « cutoff », « LFO », « seuil », « dry/wet », « decay », etc… Solution: Pratiquer en permanence l'écoute active = comprendre les effets sur le son des outils de traitements à la disposition du producteur/mixeur musical L’écoute active permet de faire le lien entre ton action dans le processing d’un traitement audio et son effet sur l’élément sonore. Tu augmenteras tes compétences en production musicale en t’entraînant régulièrement. Et c’est bien l’objectif de ce podcast! D’ailleurs, l’un des avantages du traitement analogique du son par rapport au numérique est qu’il permet de se concentrer uniquement sur l’écoute, sans aucune distraction visuelle! Souvent, dans ces débats entre analogique et numérique, c’est devenu l’un des principaux arguments de l’analogique. On peut traiter le son en bougeant des potards, faders ou autres boutons, sans la distraction de l’écran qui vient perturber notre écoute. Il faut toujours avoir conscience de cela quand tu produis dans ton ordinateur. La finalité c’est bel et bien ce qu’on écoute et pas ce qu’on voit! Précision sur l'écoute de ce podcast Petite précision avant de continuer: Les sujets abordés portant autour des traitements audio et du sound design, ce podcast sera d’autant plus enrichissant pour toi si tu peux l’écouter avec la meilleure qualité sonore possible. Tu en retireras une meilleure expérience en écoutant de manière concentrée et avec un bon casque ou tout autre système audio stéréo de bonne qualité. Alors, je sais bien que la plupart du temps, les gens écoutent les épisodes en faisant autre chose, et à l’aide des simples écouteurs du téléphone. Mais sache simplement que ce n’est pas l’idéal. Surtout quand j’aborderai des notions plus pointues en mixage audio. De plus, quelque soit le diffuseur que tu utilises pour écouter ce podcast, le fichier audio est compressé pour des raisons d’optimisation de flux et de débit, ce qui n’est pas l’idéal non plus! C’est pourquoi, je mets toujours à disposition l’épisode dans sa qualité audio optimale, non compressée. Donc, pour les plus intéressés et motivés d’entre vous, rendez-vous sur Arsonor.com dans l’article correspondant à l’épisode afin de le télécharger (tu peux télécharger l’épisode en question par le lien au début de cet article). Exemple concret: Design sonore d'un effet de montée (\"Riser\") à partir de l'analyse d'un son du quotidien Considérons un effet de montée (riser) – un classique en pop et électro. {SON} On a ici un simple bruit blanc filtré de manière assez basique et qui n’a pas vraiment d’impact … L’objectif est d’améliorer ce son pour qu’il s’insère au mieux dans une production. Pour ce faire, je vais prendre l’analogie avec le son d’une voiture qui se rapproche. Si tu ne comprends pas où je veux en venir, pas de panique, tu vas comprendre à la fin. L’idée est de montrer comment un son du réel, de notre quotidien quand il est bien analysé, s’apparente en tout point à nos décisions en production musicale. Etapes de traitement d'un son de voiture qui se rapproche Considérons ce son d’une voiture: {SON} On peut traiter ce son pour simuler le fait qu’elle se déplace vers nous: Etape 1: Sensation de déplacement de la gauche vers la droite Première chose, pour donner la sensation de mouvement du son, de son déplacement de la gauche vers la droite, je fais varier le paramètre « Panoramique ». → automation du panoramique de la gauche vers la droite (33L à 7R par exemple). Etape 2: Sensation du volume perçu Ensuite le plus évident est que le niveau perçu augmente à mesure que l’objet se rapproche. On entend le moteur plus clairement à la fin: cela se traduit par augmentation du volume, faible au début puis rapidement à son maximum à la fin). → automation d’augmentation de gain (-12 à +6 dB). D’une manière générale, les sons subtils qui ne peuvent pas être entendus de loin, peuvent être mis en évidence en utilisant ces variations de volume, et particulièrement un outil qui s’appelle la compression, ou peut-être une compression multibande. Notions qu’on aura le temps d’aborder plus tard dans d’autres épisodes. Etape 3: Sensation du contenu en hautes fréquences D’un point de vue fréquentiel, plus un objet sonore est éloigné, moins son contenu hautes fréquences est important (absorbées plus facilement par l’air) par rapport aux basses qui se propagent sur de plus longues distances. Quand elle est loin, on entend un grondement flou à basse fréquence. Donc pour retranscrire cela, on peut utiliser un filtre Low-pass qui s’ouvre progressivement vers les hautes fréquences au fur et à mesure que l’objet se rapproche. Ou alors, plus subtilement, un high shelf que je fais augmenter en gain (cutoff à 4 kHz et gain de -5 à +13 dB). Voir la série articles sur les EQ si tu ne comprends pas tous ces termes. Etape 4: Sensation de la réverbération Et enfin, une dernière étape où j’ajoute une reverb pour la sensation d’un espace particulier. La réponse en réverbération sera également radicalement différente pour les objets éloignés et rapprochés. Plus un objet est éloigné, plus le son direct nous parvenant (ligne droite sans réflexions) arrive en même temps que les autres ondes réfléchies (la reverb). Alors que quand il est plus proche, le son direct et les premières réflexions de la reverb sont plus distincts et séparables, là où avant tout était encore mélangé en un seul son réverbéré plus flou. Sur les réglages d’un plugin de reverb, cela se traduit par la modification de l’écart entre les premières réflexions et la queue de réverb. Un paramètre clé pour cela est le pré-delai. Il se règle en ms, de 0 à plusieurs 100taines (de 15 à 400 ms dans notre exemple). Ecoute A/B du son final obtenu Voilà le son final que j’obtiens: {SON} Et en comparant avec le son initial, sans traitements: {SON} On constate bien la différence entre les deux. On a bien cet effet de mouvement. Lorsque la voiture commence à nous dépasser, elle est à son plus fort. On entend mieux les sons à la fin. Application en contexte musical sur le \"Riser\" Maintenant je vais te montrer ce que cela donne sur un exemple plus musical. Il me suffit alors de remplacer le son de voiture par mon bruit blanc, le riser écouté tout à l’heure (ils font la même longueur). Du coup, en appliquant exactement tous les traitements précédents, mais cette fois ci sur ce riser à la place du bruit de voiture, voilà ce que cela donne: {SON} Ecoute la différence lorsque je bypass les traitements (désactivation): {SON} Et en activant les traitements, {SON} , le son devient déjà bien plus intéressant, il commence bas et lent, devient plus grand et plus proche, jusqu’à occuper tout l’espace des haut-parleurs. Ecoutons maintenant en contexte avec un rythme House: Sans les effets: {SON} .Et Avec: {SON} En mettant toutes ces considérations en pratique, je t’ai montré à quel point ces techniques peuvent grandement améliorer un son. Bon il existe beaucoup d’autres techniques de production et je pourrais encore largement améliorer tout ça, mais c’est pas le sujet de cet épisode aujourd’hui! Conclusion de l'épisode S’l y a une chose à retenir de cet épisode, l’équipement le plus important, c’est tes oreilles. Prends-en soin. N’oublie pas d’écouter. Toujours. Écoute le monde, écoute le film, écoute ton interlocuteur. Mais toujours dans cette perspective analytique du son et de ses propriétés physiques. Meilleur tu es dans l’écoute, meilleur tu seras en tant que producteur musical. Je te remercie d’avoir pris le temps d’écouter cet épisode de bienvenue. N’hésites pas à commenter et à partager ce podcast si cela t’a plu. De me dire ce que tu penses de ce nouveau format par exemple. Ce n’est que le début d’une longue série à venir. Je te rappelle que tu peux télécharger l’épisode en qualité audio optimale, non compressée dans l’article correspondant sur le blog Arsonor. Voilà, je te dis à très très bientôt, d’ici là, bonne écoute, bonne prod!\n",
      "article_keywords: écoute analytique, podcast, riser\n",
      "\n",
      "article_title: L’intelligence artificielle (IA) dans le studio de production audio (1/6)\n",
      "article_content: Qu'est-ce que la technologie IA dans les outils de production audio? L’intelligence artificielle (IA ou AI en anglais) n’est plus une caractéristique spéculative de la science-fiction. Elle a un effet transformateur sur une vaste gamme d’industries* et l’audio ne fait pas exception. * Selon une étude (Source: Mckinsey Global Institute), l’IA devrait générer 13 billions (mille milliards!) de dollars supplémentaires de revenus d’ici 2030. Cette technologie révolutionnaire devrait créer 58 millions de nouveaux emplois d’ici 2022 (dont la grande majorité sont des emplois que l’on ne connaît pas à l’heure actuelle) (Source: https://www.forbes.com/sites/amitchowdhry/2018/09/18/artificial-intelligence-to-create-58-million-new-jobs-by-2022-says-report/?sh=3f393ff74d4b) J’ai repéré trois grands domaines de l’audio où l’IA s’est bien implantée et fait déjà ses preuves: 1) L’IA au service de l’écoute personnalisée Cela inclut tout le domaine de la reconnaissance musicale et la recommandation personnelle (Smart Speaker, Droits d’auteur, MIR: Music Information Retrieval…) en plein boom sous l’impulsion des diverses plateformes de streaming . C’est aussi le domaine de la synthèse vocale (TTS: Text To Speech recognition) et du traitement de la parole, de dialecte ou de langue. Et enfin, cela inclut aussi le choix individuel d’un mode d’écoute au casque (émulation de sources ou d’espaces réels et spatialisation 3D). 2) L’IA comme assistant de composition et de sound design On retrouve ici tout ce qui concerne les services ou outils de génération de musique automatique (Generative Music) . Cela concerne aussi tous les nouveaux plugins d’assistance au Beatmaking (Beats machine, Générateur automatique de pattern MIDI, d’accords dans la tonalité…) , à la synthèse sonore et au son à l’image. 3) L’IA comme assistant en post-production audio On y retrouve tous ces nouveaux plugins « intelligents » dans le mixage assisté , ainsi que dans la réparation, nettoyage et séparation des sources audio. Mais aussi et enfin, cela concerne tous les services en ligne de Mastering assisté . Sommaire de la série d'articles à venir Ce qui suit dans cette série de 6 articles ne concerne que le point n°3 (je me restreint à ce thème pour ne pas faire trop long, peut être que d’autres articles suivront). A travers un tour d’horizon des outils audio actuels et en développement, ce dossier en six parties s’attache à montrer l’impact et le rôle de l’IA dans le studio de production audio: Dans cet article, j’illustre ce que représente la technologie à intelligence artificielle et montre notamment en quoi elle diffère d’un algorithme classique. Le prochain article traitera des services en ligne en Mastering audio qui est le domaine où l’IA a d’abord fait ses preuves; Le troisième article commence un état des lieux des logiciels ou plugins audio innovants sortis ces dernières années. J’aborde d’abord l’exemple de ces nouveaux « EQ intelligents » qui n’intègrent pas forcément encore de l’intelligence artificielle mais en prennent le chemin et la philosophie; La quatrième partie fera un tour d’horizon des bundles de plugins intégrant de l’IA pour le mixage et le mastering assisté , comme par exemple ceux d’iZotope ou les Smart de Sonible; La cinquième partie traitera d’un domaine particulièrement propice à l’utilisation de l’IA en post-production audio: la restauration sonore, la réduction de bruit et la séparation des sources audio . Enfin, dans le dernier article, je conclue le dossier en m’interrogeant sur l’impact de l’IA sur les métiers de la production audio et musicale , et comment je vois le secteur évoluer à plus long terme. Mais d’abord, pour bien comprendre la tendance présente et à venir, intéressons-nous à définir exactement ce qu’est une IA et son rôle dans nos outils numériques de production musicale (logiciels MAO, plugins, …). L’intégration progressive de l’IA dans nos outils audio contribue grandement à faciliter le travail du producteur/ingénieur du son. C’est du moins un objectif clair de remplacer les tâches ennuyeuses et répétitives par un logiciel qui ferait tout le « sale boulot » à la place de l’homme. Comme un assistant dévoué sur des tâches qui peuvent être automatisées. Néanmoins l’évolution du studio de production jusqu’à l’ère du numérique montre déjà cette tendance: De l’automatisation mécanique des tâches à l’arrivée du numérique Mis à part l’évolution de la qualité sonore, beaucoup d’innovations majeures ont eu aussi pour but de faciliter le travail d’enregistrement et de traitement audio. Si on remonte aux années 50-60, l’équilibrage des différentes sources sonores devait se faire dès la prise de son! Il faut attendre les années 70 pour que le métier d’ingénieur de mixage devienne un nouveau concept. Ceci sera facilité dans les années 80 par l’intégration d’outils de traitement audio (dynamique dédiée compresseur/expandeur/gate + EQ full paramétrique) sur chaque tranche d’une console analogique. La fonction « Total Recall » Automation sur les consoles SSL est une véritable révolution. On est désormais capables de sauver et restituer entièrement un mixage, autant par ses réglages fixes que par ses évolutions en cours de morceau! Les faders motorisés deviendront un élément central de l’enregistrement musical, permettant aux réglages d’égalisation et aux positions des faders d’être chargés dans un système ou un ordinateur directement connecté à la station de travail audio de mixage et appelés plus tard sur la ligne lorsque cela est nécessaire. Les consoles de plus en plus riches en fonctionnalités ont permis aux producteurs d’explorer de nouvelles techniques créatives et de donner naissance à des productions toujours plus modernes et sophistiquées. A la fin des années 90’s, le DAW (Digital Audio Workstation) , devient le centre névralgique numérique du studio. Le nombre de pistes et de traitements illimités permet alors à l’ingénieur de mix de prendre de plus en plus de pouvoir et agir directement sur la performance de l’artiste. Puis les années 2000 et 2010 verront la tendance croissante à réduire les machines au simple ordinateur. L’ère du home-studio, du « mix in the box » et de la démocratisation des outils à tout un chacun interrogent déjà sur le véritable rôle de l’ingénieur du son aujourd’hui. En quoi l’IA diffère-t’elle d’un simple algorithme: l’exemple du compresseur de la dynamique Si on prend l’exemple du compresseur , son invention a d’abord permis l’automatisation mécanique pour le contrôle des volumes . Avec cet outil, l’ingénieur du son n’avait plus besoin de suivre et régler manuellement le volume d’une source audio à la dynamique trop changeante. Depuis, l’évolution du compresseur en plugin numérique lui permet d’être un outil omniprésent et démocratisé dans le traitement de la dynamique, de l’enregistrement jusqu’au mastering audio. Pourtant, encore aujourd’hui, son utilisation reste complexe et largement incomprise par tout amateur ou novice des traitements audio. Par exemple, lorsque nous utilisons un plug-in de compression pour traiter un enregistrement vocal, le plug-in répond à nos réglages et à la dynamique du matériau de manière prévisible. Cela peut être très utile, mais son efficacité dépend entièrement de la façon dont nous configurons les paramètres de compression et notre capacité à écouter finement les différences. Un enregistrement vocal différent peut nécessiter un ratio de compression différent, voire une chaîne de traitement complètement différente. Au-delà des presets fournis que nous pouvons essayer, les plugins conventionnels ne peuvent pas nous aider à prendre cette décision. Au lieu de cela, nous nous appuyons sur nos propres expériences passées et nos compétences acquises pour décider des paramètres du nouvel enregistrement vocal. Là où l’intelligence artificielle intervient c’est qu’elle va supprimer le besoin d’un individu, ou plutôt, le besoin d’un cerveau humain à acquérir cette expérience. Potentiellement, un algorithme de compresseur IA pourrait analyser la performance d’un chanteur particulier, la comparer avec une bibliothèque d’autres performances et générer des paramètres de compression appropriés. Un logiciel IA pourrait émuler ces compétences cognitives . Plutôt que d’offrir simplement un réglage nécessitant des compétences, un compresseur basé sur l’IA pourrait déterminer par lui-même quel pourrait être le meilleur réglage. Grâce à l’analyse d’un grand nombre d’enregistrements vocaux existants, le logiciel construirait un modèle mathématique de ce à quoi ressemble une « bonne compression vocale ». Il appliquerait ce modèle à de nouveaux enregistrements en les comparant aux modèles de réglages qu’il a appris, sur la base de sa propre expérience acquise, plutôt que de la nôtre. Par exemple, un musicien n’ayant pas les compétences nécessaires pour mixer une chanson pourrait compter sur un mixage assisté par l’IA ; plutôt que d’avoir à apprendre à utiliser les commandes de compresseur telles que le ratio, le seuil, l’attack et le release, il pourrait simplement dire qu’il veut « les voix plus fortes et plus consistantes », ou que « la guitare doit être plus proéminente que le piano ». La technologie IA dans les studios aujourd’hui Le contrôle de la plage dynamique et l’utilisation de compresseurs est une modeste illustration théorique , dans une certaine mesure, de ce que peut apporter l’intelligence artificielle. Car pour que des algorithmes fassent le travail à notre place, l’IA est basée sur l’analyse de données, le Big Data . On entendra souvent les termes de « Machine learning » ou « Deep learning » et on parlera d’architectures de code DNN (Deep Neural Networks) ou encore C/RNN (Convolutional / Recurrent Neural Networks) . Ces concepts informatiques sont également applicables à de nombreux autres scénarios de production musicale et pourraient profondément changer la façon dont nous produisons de la musique. En fait, l’IA a déjà un impact significatif dans des domaines tels que le mastering et la composition. A mesure que l’assistance IA prend de l’ampleur , les créateurs et les machines continuent de s’entremêler de plus en plus, les flux de travail créatifs prenant de nouvelles formes. Dès lors, cela nous amène à s’interroger sur l’avenir du métier de producteur/ingénieur du son . Comment la technologie IA l’impacte t’elle et comment est-elle susceptible de se développer à plus long terme? Nos chers DAW vont-ils tomber en désuétude? Sont-ils déjà has been? Quel avenir pour les métiers de la production musicale, à la fois sur le plan technique et créatif? Rendez-vous la semaine prochaine pour la suite 😉\n",
      "article_keywords: algorithme, deep learning, IA, logiciel audio, machine learning, plugin, post-production\n",
      "\n",
      "article_title: 10 logiciels incontournables pour le sound design\n",
      "article_content: Le design sonore (ou plus communément appelé sound design ) est un terme un peu passe-partout employé à toutes les sauces. A partir du moment où mon métier consiste à transformer des sons enregistrés ou à en créer de nouveaux de manière synthétique, je suis un designer sonore. Cela se pratique depuis toujours dans le son à l’image et plus récemment dans toute production musicale moderne. D’ailleurs la frontière entre sound design et composition musicale s’est considérablement rétrécie. Musique ou sound design? En effet, si je conçois un son simulant un impact de balle dans une planche de bois, on appellera cela communément du sound design. Et si j’utilise ce même son pour rendre une caisse claire plus percutante, je deviens un producteur musical. Une fois accepté que tout son peut être vu comme possible entité musicale, c’est une démarche naturelle d’utiliser des outils de sound design dans le processus de production. Ces dernières années, les outils numériques de traitement du son ont incroyablement évolués et continuent d’évoluer sans cesse. Si bien qu’ils représentent aujourd’hui les moyens incontournables pour recréer des sons originaux ou leur redonner vie. Pourquoi pas travailler sur une hybridation de son analogique et numérique? Travailler avec des bruits plutôt que des notes? Et pourquoi pas essayer de rendre organique un son ne provenant d’aucune source physique? Mode de sélection des logiciels de sound design Pour faire face au choix pléthorique parmi les sociétés fabriquant ces outils, j’ai créé cette liste des 10 logiciels incontournables en me basant sur les catégories (“type” de logiciel) suivantes: les DAW : ta station audio-numérique est bien sûr ton premier et principal outil de création sonore, la base multi-fonctionnelle; les synthétiseurs virtuels (VSTi) , parmi les mastodontes du marché (y intégrant des techniques de synthèse sonore diverses); les “plugins” d’effets divers sur le traitement du son. Ces derniers ont d’ailleurs souvent pour vocation première d’être utilisés en production ou mixage audio. Mais le principe du sound design est justement de les détourner de leur fonction première. Utiliser les outils de production musicale d’une manière “non musicale” ou non intentionnelle peut mener à des résultats réjouissants et surprenants. Puis, parmi ces catégories, j’ai appuyé mes choix suivant ces critères : algorithme parmi les plus innovants et bluffants de ces dernières années; accès à des banques de son de haute qualité; dans l’air du temps, ainsi que bénéficiant d’une popularité et d’une communauté d’utilisateurs très forte; doté d’une interface agréable et un rapport d’apparente complexité/simplicité d’utilisation optimal. C’est parti! 1) Station audio-numérique (DAW) Ableton Live Est-il vraiment nécessaire de le présenter? Véritable révolution à sa sortie parce qu’il déportait le séquenceur traditionnellement dédié au studio dans l’univers du live (d’où son nom), Ableton est devenu une institution et Live le logiciel favori de quantités de DJ et musiciens électroniques qui l’utilisent sur scène ou en studio, seul ou en complément de leur setup. Ses effets tels que le Beat Repeat, l’ Autofilter ou l’ Autopan , ses instruments Simpler ou Operator ont été plébiscités dans la création d’un nombre incalculables de morceaux de ces dernières années. En matière de DAW, comme je le disais dans l’article La MAO démystifiée , on a toutefois l’embarras du choix: il en existe une multitude avec plus ou moins d’ancienneté, chacun a ses atouts et sa spécialité. Et bien là où la majorité des DAW se dédie traditionnellement à la prod et au mixage en studio, sorte de pendant logiciel à nos bonnes vieilles consoles analogiques, Ableton prend le contre-pied en le dédiant, d’abord au live, mais aussi et surtout (et par conséquent) aux expérimentations sonores les plus folles . Car ce qui sépare Live d’une autre DAW, outre ses effets et instruments natifs, c’est bel et bien son fameux mode Session : “une page d’idées”, une matrice de pads, interface unique pour improviser, jouer et performer avec des idées musicales, sans les contraintes de la ligne de temps. Quasiment tout dans Live travaille en temps réel et en synchro; ajoute, réordonne ou enlève divers effets, joue avec le routage flexible et bien plus encore, le tout sans interrompre ton flow créatif. Tu l’as compris, Live est un incontournable pour la créativité musicale et le sound design. Demo: Et enfin une vidéo datant de 2006! Eh oui même si beaucoup de choses ont évoluées entre temps, les principes de base restent toujours les mêmes: Autres recommandations: Une autre DAW très populaire pour tous les utilisateurs d’un Macbook Pro puisque rachetée par Apple, je veux bien sûr parler de Logic Pro . En matière de sound design, il y a aussi de quoi faire avec des outils intégrés de choix comme les synthétiseurs Alchemy ou Sculpture , et sa reverb à convolution Space Designer. 2) Synthèse hybride (tout en un) Spectrasonics Omnisphere Depuis 2008, la réputation de la firme Spectrasonics n’est plus à faire dans le monde du sound design et des instruments virtuels. Et leur produit phare, le synthétiseur virtuel mastodonte (ou “Power Synth”), Omnisphere le démontre bien: en plus de posséder tous les atouts d’un synthé classique à synthèse soustractive, son moteur puissant intègre (liste non exhaustive) des fonctions de synthèse FM et granulaire, un arpégiateur et des effets de haute facture . Tout ceci au sein d’une interface simple et épurée. Et c’est sans compter l’importation possible de ses propres sons en plus de sa bibliothèque interne tout simplement monstrueuse . Tu peux y reconnaître d’ailleurs la plupart des sons “hollywoodiens” que les designers sonores pour les films utilisent abondamment. Eric Persing, le boss de Spectrasonics, a su en effet s’entourer d’une ribambelle de sound designers de génie (Diego Stocco, Ignacio Longo, Richard Devine pour ne citer qu’eux) pour te fournir des échantillons sonores d’une qualité incomparable. Ce joujou a d’ailleurs le défaut de ses qualités car il t’en coûtera pas moins d’un espace de 64GB de sons à réserver sur ton disque dur si tu veux en faire l’acquisition. Et les sons incorporés sont tellement de qualité et utilisables tels quels que tu en oublierais presque qu’il est aussi possible d’utiliser ses fonctions pourtant infinies de synthèse sonore. Demo: Un des sound designer de renom à avoir grandement participé à l’élaboration des sons d’Omnisphere n’est autre que Diego Stocco. Qui mieux que lui pour présenter ce que le synthé a dans le ventre: Autres recommandations: – Alchemy (anciennement produit de chez Camel Audio, il a été racheté par Apple et totalement intégré dans Logic Pro! Une raison majeure de se procurer Logic si t’hésites encore) – KV331 Audio Synthmaster – Synapse Audio Dune 3 3) Synthèse à tables d'onde (wavetable) Xfer Records Serum Si j’en parle dans une section à part, c’est que ce type de synthèse a le vent en poupe. Depuis le Wave de PPG (créé en 1981 par Wolfgang Palm) et de Waldorf , la synthèse à table d’ondes s’est peu à peu popularisée sous forme numérique, notamment par l’intermédiaire de Native Instruments avec Massive . Mais c’est surtout Steve Duda et son instrument Serum qui s’est définitivement imposé comme LE synthétiseur virtuel à tables d’ondes . Ce type de synthèse a conquis depuis bon nombre de producteurs de musiques électroniques. Mais pas que. Car même si cela peut sembler destiné à des sons froids et synthétiques c’est que tu n’as pas bien saisi toutes les possibilités offertes par le logiciel. Cette façon de pouvoir importer et “sculpter” le son à sa source, c’est-à-dire dans sa forme d’onde, a un potentiel créatif énorme et original. Demo: Un exemple avec l’utilisation avancée des tables d’ondes: Autres recommandations: – Ableton Live Wavetable (Dans sa version 10, Live intègre son nouvel instrument à table d’ondes. L’idée de concurrencer Serum sur ce terrain est évidente et il ne s’en cache pas). – Arturia Pigments (La firme française Arturia s’y met aussi avec son nouveau bébé) – Native Instruments Massive X : Jusqu’à aujourd’hui, Massive est toujours l’un des synthés virtuels de référence sur le marché, malgré son interface vieillotte et quasi-inchangée depuis sa sortie. Sauf qu’en 2019, NI s’est enfin décidé à le relifter avec une mise à jour majeure sous le nom de Massive X (qui est en fait un nouveau synthétiseur à part entière qu’une simple mise à jour). 4) Sampling Native Instruments Kontakt Le sampling est une technique de production maintenant connue depuis plus de 30 ans, ayant défini beaucoup de tendances musicales et même façonné des genres entiers. Après la révolution des machines hardware Akai , le sampler d’aujourd’hui est un instrument virtuel permettant d’y charger des bouts d’audio (samples) et les jouer à l’envie à différents pitchs et vélocités . J’arrête là pour la partie “technique” car c’est pas le sujet de l’article. Les évolutions techniques du sampling permettent de nos jours de pouvoir recréer (ou plutôt émuler) n’importe quel instrument de manière très réaliste, mais aussi de manipuler les échantillons sonores dans tous les sens, ce qui en fait un outil de choix pour le sound design . Et la particularité de Kontakt, c’est que nombre pléthorique de marques d’émulation d’instruments réels et leur librairie d’échantillons sonores s’intègre directement dans le logiciel. Le “player” gratuit Kontakt peut être utilisé pour pleins d’instruments. Mais la plupart des instruments provenant de sociétés tierces exigent la version complète. Si tu es intéressé par le sampling en tant que producteur ou compositeur, Kontakt est un “must have” puisqu’il permet d’accéder à la majorité des librairies disponibles sur le marché. C’est ce qui en fait le sampler le plus utilisé au monde et un statut de quasi indispensable dans le monde de la production musicale numérique. Demo: L’utilisation de Kontakt étant plus connue en jouant sur des instruments venant de société tierces, voilà un tuto montrant les fonctions avancées du sampler en lui-même afin de créer son propre instrument: Autres recommandations: Il y a des tonnes d’autres choix en matière de sampler dont bien évidemment: – Celui de ta DAW pourrait aussi très bien faire l’affaire ( Steinberg Cubase Halion, Ableton Live Simpler + Sampler, Logic EXS24 …) – UVI Falcon : le constructeur français, responsable de beaucoup d’innovations dans le domaine, mérite d’être mentionné. 5) Programmation modulaire Native Instruments Reaktor Ne t’y trompe pas, on ne parle pas ici d’un simple instrument mais plutôt d’une sorte de méga interface dont tu es le héros ! Comme Kontakt, Reaktor a en commun ce concept de script ou programmation sonore personnalisée : des développeurs tiers (ou toi-même!) peuvent y créer des interfaces et instruments uniques. Si tout cela te rebute car tu n’es pas programmeur, rassure toi, la dernière version intègre un système de “blocks” très facile à utiliser. Et si cela t’effraies toujours, tu peux simplement t’en servir en chargeant l’un des innombrables modules à ta disposition. Tu t’apercevras assez tôt que les journées défilent très vite pendant que tu tritures les boutons de manière aléatoire, tout en te régalant les oreilles. Car même remarque ici que pour Omnisphere précédemment : Reaktor intègre déjà un large choix d’instruments utilisables tels quels qui sont d’une telle qualité et originalité qu’il est facile de s’en contenter! Cela va du synthé, générateur de sons aux boîtes à rythme et autres séquenceurs et effets. L’autre gros point fort de Reaktor est sa communauté d’utilisateurs très active où chacun peut y aller de son propre instrument et le partager dans une sorte de librairie accessible ici: https://www.native-instruments.com/fr/reaktor-community/reaktor-user-library/ . Bref, un MUST! Demo: Et un exemple de ce qu’il est possible d’y faire entre autres choses: Autres recommandations: – Max/MSP : Développé par l’Ircam dans les années 80, il est commercialisé aujourd’hui par la société Cycling’74 et surtout, il est complètement intégré dans Ableton Live (voir précédemment)! Comme pour Reaktor (qui en est donc un concurrent direct), la communauté d’utilisateurs très active peut créer des “Max for Live Devices” (M4L) ; ce qui décuple encore les possibilités sur Ableton et en font définitivement le logiciel n°1 en sound design (voir point 1)). Voir ici pour un aperçu de toutes les extensions Max for Live: https://www.ableton.com/en/packs/#?item_type=max_for_live . – Pure data : Développé en parallèle à Max/MSP, le code est en Open source et gratuit. C’est donc un logiciel de programmation graphique très populaire pour la création musicale et multimédia en temps réel. 6) Correction tonale, Pitch-Shift et Time-Stretch Celemony Melodyne Le “pitch-shift” (changer la tonalité d’un son sans en altérer sa vitesse) et son corollaire le “time-stretch” (changer la longueur d’un son sans en altérer sa tonalité) sont des procédés à la base de l’évolution des technologies audio-numériques. Tu utilises leurs algorithmes associés dans toutes les sources audio sans t’en rendre compte (pour la synchronisation du son à l’image à la moindre application audio d’aujourd’hui en passant par l’art du DJing bien entendu). Du côté des studios musicaux, cela a permis de révolutionner le travail (en bien ou en mal) jusqu’à être capable de faire passer n’importe qui chantant comme une casserole à un ténor d’opéra prestigieux (j’exagère à peine). L’outil magique permettant de faire cela c’est l ’autotune , nom originaire du produit phare de chez Antares . Mais depuis peu un autre concurrent sérieux s’est mis sur ce marché; je veux bien sûr parler de Celemony avec son Melodyne . Tous les studios musicaux professionnels du monde entier ont dans leur arsenal au moins l’un ou l’autre (si ce n’est pas les deux). Mais alors qu’est-ce qui fait que je parle de Melodyne dans cet liste d’outils pour le sound design? Contrairement à l’Autotune d’Antares, Melodyne ne fait pas de correction de pitch en temps réel (pendant que le chanteur chante) mais s’utilise en post-production. En revanche, son concept et ses algorithmes sont révolutionnaires. Son concepteur, un luthier et mathématicien allemand de génie , a imaginé la transposition visuelle de l’audio (habituellement sous forme d’onde sonore) note par note, chacune représentée par une forme de “goutte” . Là où jusqu’à maintenant on se contentait de manipuler l’onde sonore, avec Melodyne, on plonge littéralement à l’intérieur du son ! La correction de note en tonalité ou en durée devient un jeu d’enfant. De plus, son éditeur de son (dans la version complète du logiciel) ouvre la voie à des possibilités extraordinaires. Demo: N’hésites surtout pas à parcourir le site officiel car on y retrouve des tutos également disponibles en français (ce qui est très rare)! En voilà un parlant justement de l’éditeur de son: Présentation de l’éditeur par le Maestro himself (mais en anglais seulement): Et enfin, un bon exemple de ce qu’il est possible de faire avec une voix chantée: Autres recommandations: – Antares Autotune : Si tu n’as encore jamais entendu parlé d’Autotune, c’est que le rap/Hip Hop et toi, ça fait deux! LE concurrent de Melodyne, tout simplement. Il n’est pas aussi poussé dans le traitement note par note mais il se différencie par son traitement du pitch en temp réel! – Serato Pitch’n’time : Peut-être le meilleur pitch-shift du marché. – PaulXStretch : En voilà un logiciel de time-stretch bien original, à l’algorithme révolutionnaire et surtout gratuit! Si je devais garder un seul coup de coeur de toutes ces listes de plugins ce serait celui-là. Il avait fait le buzz à l’époque de sa sortie grâce à son utilisation sur une chanson de Justin Bieber qui la transforma d’une longueur de 3 min 21 en une voix de fantôme sous amphétamines de 35 minutes! (1.8 millions de vues sur Youtube en une semaine). Il est devenu depuis un outil original pour créer ou transformer n’importe quel son en nappe ambient bien planante. Voici un extrait du morceau: Et voici un exemple d’utilisation édifiante (le plugin a depuis évolué avec une interface plus moderne): 7) Restauration sonore Izotope RX Après un logiciel de correction tonale, voilà maintenant un logiciel de restauration sonore? Oh que oui! En quelques années, l’ensemble de plugins RX de Izotope s’est imposé comme LA référence dans le domaine de la réparation audio (enlever les clics, le clipping, les bruits de fonds intempestifs, et bien plus maintenant…). Au point de détrôner peu à peu l’outil phare qui était spécialement dédié à cela jusque là: l’ensemble Cedar . Mais ce qui fait la spécificité de RX, c’est son interface (en “standalone”, c’est-à-dire en utilisant le logiciel seul, pas sous forme de plugin) visuelle sous la forme cette fois d’un spectrogramme . Un quoi? Outre la forme d’onde ou le spectre, le spectrogramme est la représentation visuelle du son permettant d’analyser à la fois son contenu fréquentiel et dynamique en fonction du temps (temps en abscisse, fréquences en ordonnée et intensité du son en surbrillance colorée). Et les outils remarquables de RX permettant de modifier ce spectrogramme, ajouté aux nombreux modules disponibles pour altérer le son, en font un outil en or pour tout sound designer. La dernière mouture de RX est LE logiciel, avec Melodyne, permettant notamment de s’approcher de plus en plus vers la possibilité de séparer les éléments audio d’un morceau (séparer l’acapella de l’instrumental par exemple). Demo: Autres recommandations: – Accusonus Era Bundle – Zynaptiq Un-Series 8) Reverb/Delay Zynaptiq Adaptiverb Dans la catégorie des plugins détournés de leur fonction première dans le mixage audio, il faut bien sûr parler des reverbs et delays. Les algorithmes de ces outils ont bien évolués et certains plugins permettent un rendu dans l’espace de grande qualité. Dans le domaine du sound design, le travail dans l’ espace sonore est primordial. Ici l’objectif n’est pas le réalisme mais plutôt la recherche d ’ambiances éthérées et irréelles . Pour cela, l’Adaptiverb fait fabuleusement le job. Ce n’est d’ailleurs pas exactement un plugin de reverb à proprement parlé, mais plutôt une sorte d’hybride synthétiseur/reverb s’adaptant automatiquement au contexte audio. Demo: Dans cette catégorie des “reverbs à effets spéciaux” , j’aurais pu tout aussi bien citer celles-ci: – Eventide Blackhole – 2CAudio Aether – Valhalla Shimmer – Surreal Machines Diffuse Il faut bien sûr aussi parler de la catégorie des Reverbs à convolution : car même si leur objectif premier est un rendu le plus réaliste possible, on peut y importer ses propres IR (impulsions audio), et alors tout devient possible. – AudioEase Altiverb : la Rolls Royce des reverbs à convo – Ou celle de ta DAW si elle en a une ( Space Designer sur Logic, Convolution Reverb Pro avec Max for Live, etc, etc…) Enfin, on peut mentionner dans cette section la catégorie des Delays extrêmement créatifs et originaux : – Fabfilter Timeless – Soundtoys Echoboy – Boz Digital Labs Imperial Delay – MeldaProduction MSpectralDelay 9) Saturation Izotope Trash A l’instar de la Reverb, la saturation est à priori un procédé utilisé en mixage audio. La saturation a différents effets sur le son: en plus d’ajouter différentes types de couleur et d’harmoniques, cela va aussi influer sur l’équilibre spectral et dynamique. Trash de chez Izotope est l’un de ceux qui élève l’effet de saturation et de distortion à un nouveau niveau de destruction sonore. Mais surtout c’est l’un des rares à pouvoir s’utiliser presque plus comme un instrument que comme un simple effet. Demo: Autres recommandations: Autres plugins forts recommandables aux doux noms évocateurs: – Soundtoys Filterfreak / Decapitator – Fabfilter Saturn – D16 Devastor – Ohm Force Ohmicide 10) Transformation vocale Krotos Reformer Pro Et l’algorithme le plus innovant de ces derniers temps nous vient de la société écossaise Krotos Audio . Grâce à sa technologie d’entrée audio dynamique , Reformer Pro te permet de performer avec ta voix n’importe quel type de son en temps réel. Par exemple, tu veux un rugissement de lion féroce pendant qu’on voit le lion rugir à l’image? eh bien il te suffit de grogner toi-même dans un micro, et le son en sortie (à l’aide d’une librairie de sons bien confectionnée) rendra le rugissement hyper réaliste et suivant en temps réel les mouvements de tes cordes vocales. L’outil rêvé pour concevoir des sons à l’image! Reformer Pro révolutionne le travail de l’artiste “Foley” . Ce sont ces bruiteurs du cinéma qui performent en temps réel tous les sons des objets et personnages en mouvement à l’aide d’une multitudes d’objets divers à leur disposition. Avec Reformer Pro, c’est la même idée sauf qu’il te suffit maintenant d’un bon micro et de banques de son bien construites. De multiples banques sont d’ailleurs en vente sur le site du constructeur mais tu peux aussi importer tes propres sons (en suivant quelques règles). Alors tu me diras, wow c’est super pour le son à l’image, mais pour le sound design en création musicale? Eh bien c’est pareil! Les banques de sons peuvent aussi réagir à l’audio enregistré sur ta DAW (pour des percussions ou autre) mais également être contrôlées par Midi. Dans la création musicale moderne, beaucoup d’effets sonores (Foley ou FX) s’adaptent magnifiquement dans un contexte rythmique ou musical. La richesse spectrale et la complexité rythmique inhérente à ces sons est souvent sous-estimée. En plus de Reformer pro, Krotos a également dans sa palette les outils Dehumanizer et Weaponizer qui sont dans la même veine et valent aussi le coup. Demo: Autres recommandations: Sous le terme “transformation vocale”, on peut y retrouver d’autres types de technologie audio: – AudioEase Speakerphone : Comment ne pas penser aux maîtres de la convolution avec la société hollandaise AudioEase . Très connus pour leur reverb phare Altiverb , Speakerphone est aussi un outil très utile pour transformer une voix “normale” en une voix sortant de n’importe quel endroit (appareils électroniques, mégaphone, etc, etc…) – Zynaptiq Wormhole : Grâce à une combinaison de “warping” spectral, d’algorithmes de reverb riche, de pitch-shifting et de morphing , Zynaptiq (encore eux) permet avec Wormhole d’obtenir rapidement des ambiences surréalistes ou des voix d’aliens, monstres et robots en tout genre. – Celemony Melodyne : voir plus haut – Flux Ircam Trax : Ce plugin est absolument époustouflant! Il permet, par exemple, de passer d’une voix d’homme (ou femme) à une voix d’enfant ou de personne âgée avec un réalisme rare! – Izotope Vocalsynth : Izotope n’est aussi pas en reste avec son vocoder du futur. 11) En bonus: Les bundles Si tu ne devais choisir qu’un seul pack de plugins pour le sound design venant d’un seul fabriquant, alors tu peux te tourner vers ce que les marques appellent un Bundle. A savoir, plusieurs plugins regroupés sous forme de “Pack” pour un prix plus avantageux qu’à l’unité. – Native Instruments Komplete : C’est le pack ultime avec (quasi) tout NI à l’intérieur. Attention toutefois à l’indigestion! – Fabfilter : très connu pour ses plugins de mixage et notamment son EQ Pro-Q, il n’est pas en reste comme on l’a vu dans des outils de choix pour le sound design (Timeless, Saturn, Volcano…) – Soundtoys : idem qu’au-dessus avec entre autres son delay qu’on ne présente plus Echoboy, son saturateur Decapitator, Crystalliser, … – Izotope : idem qu’au dessus, outre ceux déjà cités dans l’article, Iris, Stutter Edit et Breaktweaker méritent le coup d’oeil. – Sugarbytes : Cette marque est l’une des plus intéressantes en matière de plugins d’effets en tout genre (Turnado, Wow, Effectrix…) Voilà, j’ai essayé d’être le plus exhaustif possible sur le sujet. Toutefois, le marché est tellement vaste! De nouveaux constructeurs débarquent en permanence avec des outils tous plus passionnants et innovants les uns que les autres. Toi aussi tu as sûrement tes préférences alors n’hésite pas à écrire dans les commentaires ce que je n’aurais pas mentionné ici! Attention! Remarque importante si tu es débutant : comme je l’avais mentionné dans cet article , il devient vite contre-productif de se laisser tenter dans la course aux plugins. Contente toi d’abord de maîtriser ta DAW de fond en comble; elle possède déjà une foule d’outils largement recommandables. A bon entendeur!\n",
      "article_keywords: plugin, sound design\n",
      "\n",
      "article_title: Amen Break Beatmaking: mise en pratique dans Ableton Live\n",
      "article_content: Je fais la démonstration pratique dans Ableton Live de techniques de sampling et de Breakbeat à partir du Amen Break original. Cet article fait suite à l’épisode de podcast « Amen Break (Part 1): Les bases du Breakbeat avec la boucle la plus samplée de l’histoire » à écouter sur le lien suivant ou ici, en vous abonnant sur la plateforme de votre choix . J’y explique notamment: deux méthodes radicalement différentes pour exploiter une boucle audio à divers tempo ; la partie du Amen Break avec ses coups les plus caractéristiques et les multiples patterns rythmiques qui en découlent; deux méthodes de sampling dans Ableton; les traitements audio additionnels couramment utilisés. Retrouvez cet article en vidéo pour une meilleure illustration du process: Et aussi en podcast! Pour écouter cet épisode, lance tout simplement le lecteur ci-dessous. Tu peux aussi accéder à la page du podcast . Contenu de l'épisode #4/Vidéo: Les DAW modernes d’aujourd’hui comme Ableton Live nous permettent de facilement se ré-approprier ce type de Break en le modifiant à notre guise pour en obtenir une rythmique toute neuve. On va voir maintenant comment on procède dans Ableton. Voici le sample Amen Break , brut de décoffrage et d’une longueur d’un peu moins de 8 secondes: Création d'une boucle de deux mesures L’échantillon n’est pas encore mis sous forme d’une boucle. Son tempo original est à 137.63 BPM pour être précis. Donc si je mets le tempo du projet (arrondissons) à 138, on voit que la longueur de l’échantillon change. Ce n’est pas l’échantillon qui a changé de vitesse mais bien la grille qui s’est adaptée. Il est cette fois bien calé à la grille. On peut le vérifier en enclenchant le métronome et en prenant ce premier coup de grosse caisse au tout début d’une mesure. Pour créer la boucle de deux mesures, je décide de le faire démarrer à cette mesure-là et la finir là. Puis j’enclenche le mode boucle en cliquant sur le clip puis Ctrl L (ou CMD L). Méthode 1: Changement naturel du Pitch avec le tempo De manière naturelle (c’est-à-dire sans appliquer d’algorithme de time-stretch ou ce qu’on appelle le mode Warp dans Ableton ), la hauteur (le pitch) de cette boucle est directement dépendant de la longueur de celle-ci (comme on l’a vu) et donc du tempo de mon projet. Par exemple, si je décide de faire un morceau à 138 BPM qui est le même tempo que le Amen break original, je garde donc la boucle à son tempo original et donc son pitch naturel. Maintenant, si je décide de la jouer à un tempo plus lent, par exemple parce que je veux produire un morceau de Hip Hop avec, ce que je peux faire alors c’est diminuer son pitch ici, par exemple de 3 demi-tons. On remarque immédiatement que la longueur de boucle augmente: Et on entend deux choses: 1) elle joue plus lentement et bien sûr à une tonalité plus grave. Dans le cas du Amen Break, cela donne une meilleure assise du Kick, de manière naturelle. 2) elle n’est plus du tout calée à la grille. Pour y remédier, je réajuste le tempo du projet de manière à ce que la boucle dure à nouveau deux mesures exactement. —> On constate donc qu’une diminution de 3 demi-tons ramène le tempo à 116 BPM , comparé aux 138 BPM de la boucle originale. On peut faire le test de diminuer d’un octave ou 12 demi-tons , ce qui donne un tempo de 69 BPM. Soit la moitié de 138 BPM. Donc il y a bien cette règle d’un rapport de 2 du tempo correspondant à un octave. A l’inverse, en augmentant de 3 demi-tons, sa longueur diminue et joue donc plus rapidement: En réajustant la boucle sur deux mesures, on voit qu’on joue à un tempo de 164 BPM, plus conforme au style Jungle/Drum&Bass. Et bien évidemment le Amen break joue plus aigüe et donc aussi avec une perte dans les basses fréquences que je devrai compenser par l’ajout d’une sub bass. On peut vérifier la correspondance du tempo x2 (276 BPM) en augmentant d’un octave (+12 st). Ce qu’on vient de voir est une première option de travail avec ce genre de boucle audio. Elle a ses avantages mais aussi des inconvénients. Avantages de la méthode 1 Pas de dégradation du signal audio En effet, je n’ai fait aucun changements dans l’audio original. Quand je change le tempo, je change la longueur de l’échantillon mais en aucun cas je dégrade le signal audio qui reste parfaitement le même. On verra que c’est pas du tout le cas en appliquant du time-stretch (le Warp dans Ableton) pour faire en sorte que l’audio ne change plus de longueur avec le tempo. Et donc le deuxième avantage qui en découle, c’est qu’on conserve le groove naturel de la boucle . Le groove c’est-à-dire le timing exact de tous les coups de batterie qui composent ce break. On peut voit par exemple ici, le début de la deuxième mesure n’est pas exactement calé sur la transitoire. Et ce quelque soit le pitch auquel je vais le jouer. C’est en effet ce qui peut donner envie d’utiliser dès le départ un break en particulier, le fait qu’il apporte un certain swing à la rythmique. C’est bien dans les cas où je ne veux pas modifier l’essence-même de cette boucle, son groove. Que je peux d’ailleurs extraire, et l’appliquer ensuite sur d’autres parties programmées de mon projet. Clic droit —> Extract Groove Cela inclut les informations de timing et de vélocités de chaque coup de batterie. Inconvénients de la méthode 1 Tempo du projet fixé définitivement Comme la boucle change de longueur avec le tempo, je suis obligé de fixer le tempo de mon projet dès le départ et ne plus y toucher. Avec cette méthode, je fais de ma boucle le maître de la rythmique. Mais si je veux qu’elle joue plus lentement et donc plus grave, je change en conséquence le tempo de mon projet. Suppose de connaître son tempo d’origine Je savais que le Amen Break original joue à 138 BPM, ce qui m’a permis de caler immédiatement la boucle à la grille en indiquant le même tempo pour le projet. Editing complexe et chronophage pour changer le groove Si jamais, je veux changer ce groove et recaler certaines transitoires, je dois passer par de l’editing fastidieux dans la vue arrangement. Voilà pour la première option de ce qu’on peut faire avec une boucle rythmique dans un projet musical. Maintenant on va voir une autre manière de travailler avec cette boucle pour pouvoir l’utiliser autrement: – Cela peut être parce que j’ai déjà une rythmique en place, basée sur d’autres parties MIDI programmées. Auquel cas je veux parfaitement synchroniser ma boucle avec le reste et pas l’inverse. – C’est aussi dans le cas fréquent où je veux modifier complètement la rythmique et créer d’autres modèles plus personnalisés. Ceci dit, rien n’interdit aussi de tirer parti du meilleur des deux mondes en quantifiant ma boucle à la rythmique imposée puis de la laisser libre et intacte dans des parties du morceau plus épurées, lorsqu’elle joue seule par exemple. Méthode 2: Calage de la boucle et détermination de son tempo (mode Warp enclenché) Dans cette deuxième méthode, je vais vouloir dès le départ synchroniser et quantifier ma boucle à une grille bien précise. Donc en gros je fais l’inverse: j’adapte la boucle à une grille existante plutôt que d’adapter la grille à ma boucle. Admettons aussi que je ne connaisse pas le tempo de mon échantillon. Mon audio joue à un certain BPM (qui est en fait 138) mais je commence avec le tempo du projet à 120 par défaut. Maintenant, j’enclenche le mode Warp du clip audio. Cela aura un autre nom si vous utilisez un autre DAW qu’Ableton mais le principe est le même: c’est de synchroniser l’audio à un tempo maître par l’intermédiaire d’un algorithme de time-stretch permettant de modifier le moins possible le timbre de l’audio d’origine quelque soit le tempo maître choisi. On remarque tout de suite que le clip audio ne change plus de longueur et la lecture s’adapte parfaitement aux changements de tempo. La deuxième chose c’est que maintenant, dans la vue du clip audio, Ableton a analysé automatiquement où se trouvent les transitoires (chaque coup de percussion). En double-cliquant dessus, je crée un marqueur Warp . Cela me permet de déplacer l’audio exactement à partir de ce point-là. Faites-vous l’image de la forme d’onde audio comme un élastique et le marqueur Warp comme une épingle que l’on vient planter dans l’élastique pour le fixer exactement à ce point là. Ou alors je peux déplacer ce point exactement où je veux avec l’audio de part et d’autre qui s’étire ou se compresse en conséquence. Créons maintenant une boucle de deux mesures, la même que tout à l’heure. Et par la même occasion, cela va nous permettre de déterminer le tempo réel de l’audio qui est pour l’instant à 120 par défaut. On se rappelle que le point d’origine représentant le premier beat de la première mesure , se situe sur ce Kick. J’enclenche aussi le métronome . Pour cela, je crée le marqueur Warp à ce point-là. En zoomant je peux voir qu’il n’est pas exactement aligné au début de la transitoire. Après l’analyse par Ableton, il faut parfois repasser derrière manuellement et réajuster les points Warp plus précisément. en cliquant sur le marqueur et en maintenant Shift appuyé A ce marqueur Warp, je fais Clic droit —> Set 1.1.1 here pour indiquer que c’est bien là le premier temps du début de ma boucle. Le 1 de la grille s’est bien calé à ce point. Maintenant je détermine le début de la deuxième mesure, qui se trouve en fait à ce Kick-là (compter les temps) . Je crée donc la marqueur Warp à ce point et je le déplace sur le 2 de la grille. Enfin, je détermine la fin de la deuxième mesure de la boucle en tâtonnant un peu et en m’aidant d’un métronome. J’obtiens par exemple une boucle de deux mesures prise au milieu du Break: elle sonne comme ceci: Ensuite, pour faire de mon break une boucle qui peut tourner en continu, j’enclenche le mode Loop . Je peux ensuite isoler la boucle du reste de l’échantillon (dans Ableton, cela s’appelle « Crop sample » ): cela me crée un nouveau fichier audio de la boucle seule. Mais c’est bien sûr un procédé non-destructif car le break original entier est conservé dans mon dossier. Tests à différents tempos et algorithmes de Warp A 138 BPM qui est le même tempo que la boucle originale, il n’y a aucune dégradation du son quelque soit le mode de warping (ou algorithme de time-stretch) choisi. En testant comment sonne la boucle à différents tempos, je détermine l’algorithme de time-stretch le plus approprié. —> J’ai constaté que le Mode Complex dans Ableton semble le meilleur pour les tempos plus lents ( Boucle à 110 BPM) —> Et le mode Beats pour les tempos plus rapides ( Boucle à 160 BPM) Création d’une deuxième boucle extraite du Amen Break Toutefois, je vais prendre une autre boucle, plus intéressante pour la suite de la démonstration. Elle se situe dans la partie finale du Break, là où se situent tous les coups de batterie les plus uniques: le Kick suivi de la cymbale Ride la cymbale crash suivie de la snare et ce double roulement de snare entre les deux. C’est de loin la partie du Amen Break la plus samplée dans les genres Jungle/Drum’n’Bass. En suivant la même méthode que précédemment, je finis par isoler la boucle après avoir quantifier les principaux coups sur la grille. Cela donne une boucle longue d’un peu moins d’une mesure et demi: Elle convient bien à un tempo rapide. Je passe donc mon projet à 170 BPM . Une fois que notre boucle est prête, on va passer au sampling proprement dit. L’idée est de pouvoir jouer chaque coup de batterie composant la boucle individuellement, chacun assigné à un pad. On va donc passer la boucle audio en piste MIDI où il sera alors facile de jouer chaque coup à volonté et dans l’ordre que l’on veut. Pour cela, il y a deux façons de faire dans Ableton: Echantillonnage de la boucle avec l’instrument « Simpler » d’Ableton La première méthode est de charger directement la boucle dans l’instrument « Simpler » qui est l’équivalent d’un sampler qui va intégrer un seul sample ou boucle audio. Puis le mode Slice permet de découper automatiquement l’audio suivant ses transitoires. Sinon il y a aussi possibilité de le faire manuellement. Maintenant, si vous jouez des notes MIDI à partir d’un clavier ou des pads, les notes à partir de C1 déclencheront les différents Slices. Cela permet facilement de se créer un nouveau rythme tout en conservant les caractéristiques sonores de l’original, et à n’importe quel tempo. Activez Poly dans Simpler pour pouvoir déclencher plusieurs coups de batterie en même temps. C’est un moyen rapide d’obtenir un rythme d’un échantillon à l’autre, prêt à être utilisé dans votre musique. Une autre astuce est de découper par Beat (1/8: à la croche) et d’activer le mode Thru . Avec ce mode, l’échantillon continue de jouer après avoir déclencher un slice, jusqu’à ce qu’on enclenche un nouveau slice. On peut alors se préparer un clip Midi jouant d’abord la boucle normalement. Pour cela, chaque slice correspondant à une longueur de 1/8 (à la croche) est disposé l’un à la suite de l’autre (quantifiez la grille à 1/8). Expérimentez différents patterns rythmiques en déplaçant les slices verticalement sur la grille (avec la quantification à 1/8, puis tester aussi à 1/16). Echantillonnage de la boucle par le découpage en une nouvelle piste MIDI La deuxième méthode est de découper la boucle en la transformant en Drum Rack. Avec un Clic droit sur le clip, je choisis de « découper vers une nouvelle piste MIDI » Option de longueur de découpe – suivant les transitoires – suivant les markers – suivant une longueur de note Tout à l’heure, ça marchait bien par beat division 1/8, mais cette fois je vais tenter de découper par toute les transitoires présentes. Une nouvelle piste Midi est créée automatiquement à côté: avec un clip qui contient une note pour chaque slice et un Drum Rack avec cette fois un Simpler pour chacun des Slices. On enclenche les différents slices à partir de C1 comme tout à l’heure. Quand on lance le clip, je vérifie qu’il est bien lu correctement. Si ce n’est pas le cas, il faut sûrement ajuster les positions de lecture de chaque slice. Macros du Drum Rack Utilisez les macros du Drum Rack incluant des commandes pour « Start Offset » et « Loop Length ». Si on entend des clics intempestifs, c’est que le mode Loop est enclenché —> désactivez-le. Cliquez sur le bouton Afficher/Masquer à gauche du rack, ou double-cliquez sur le pad de batterie concerné, si vous ne voyez pas les Simplers. Ensuite, il est parfois nécessaire d’affiner l’alignement des notes sur une précision d’une croche (1/8) ou d’une double-croche (1/16) pour les roulements de la snare. Création de Pattern rythmiques Comme avant, on peut re-séquencer ce rythme, tout en gardant le son d’origine. – En déplaçant les notes horizontalement, je déclenche le son à d’autres positions. – En déplaçant les notes verticalement, je change le type de son à cette même position. Je peux bien sûr changer la longueur de mon clip. Je le mets sur deux mesures pour commencer. Je vous fais écouter finalement des ré-arrangements possibles sur 2 ou 4 mesures: Traitements audio additionnels Ensuite, plusieurs traitements s’imposent, à commencer par un EQ. 1) Traitement EQ a) High Pass à environ 100Hz, Q 0,6 car le low end sera apporté par d’autres éléments plus tard (voir étape du Kick layering). b) boost du High end pour l’air, crisp: +3dB à 3000 Hz c) boost des fréquences de la snare parfois avec un Q élevé, pour se restreindre à une fréquence précise à mettre en valeur +3 dB à 1800 Hz + 2,5 dB à 300 Hz d) creuser légèrement le low-mid ( -5dB à 700 Hz , son boxy boîte creuse) e) A/B de l’ensemble du traitement EQ 2) Jouer avec l'enveloppe d'amplitude Ajuster le sustain et le decay de l’enveloppe d’amplitude permet de mieux définir chaque coup percussif. – Tester de mettre le Sustain à -inf. – Puis faire varier le Decay du minimum juqu’à une valeur suffisante (ici à 550-600 ms). – Enfin, remonter un peu le sustain. 3) Modifier le pitch Si vous voulez retrouvez une vibe un peu vintage, ça peut être pas mal de changer le pitch original, selon le tempo du projet. a) Changer la transposition d’un slice et clic droit —> copy value to siblings Faire écouter avec le Pitch de chaque slice +3st ou -3st b) Attention il faut peut être revenir aux réglages de l’EQ après le changement de pitch! 4) Layering du Kick a) Choisir un Kick avec plus de sub b) l’aligner avec les coups de Kick de ma boucle samplée c) ajuster les niveaux 5) Distorsion De la disorsion ajoute du caractère ou remet en avant certains coups de HiHats. a) parcourir les presets de votre plugin de disto favori pour un changement total de caractère. b) tester d’activer l’EQ avant ou après la distorsion dans la chaîne du signal. c) choisir un preset et ajuster le Dry/Wet du plugin pour un effet plus subtil. 6) Compression Enfin, un traitement de compression sur l’ensemble Kick/Break permet d’unifier le tout et modifier légèrement le timbre et loudness final voulu. a) seuil à -18 dB pour ne prendre que les pics (Percussive « Pah », « Keuh » du Kick, Snare, Charley) b) ratio à tester à fond si on veut cet effet squash ou pompage (en baissant aussi le seuil et l’attaque à 0). Sinon ramener à un niveau intermédiaire c) Attack: laisser passer les transitoires pour le punch (jusqu’à 30 ms environ) d) Release: l’augmenter un peu pour laisser respirer le son en rythme e) enlever le makeup (car souvent trop fort) et ajuster l’output manuellement Résultat et comparaison finale —> Faire une comparaison entre la boucle brute et le résultat final. et ajout d’une basse pour le contexte. Les possibilités de traitement sont illimitées! Et ce que j’ai montré sur l’ensemble des « slices » peut aussi bien être effectué sur un seul « slice » en particulier. Avec des effets de filtre ou de reverse, particulièrement efficaces également. Outro de l'épisode\n",
      "article_keywords: ableton live, amen break, boucle audio, breakbeat, pitch, sampling, time-stretch, warp\n",
      "\n",
      "article_title: Les bases du Breakbeat avec le Amen Break\n",
      "article_content: Voici l’épisode 3 de la saison 2 du podcast Arsonor! Cet épisode raconte les origines et évolutions du Breakbeat en tant que technique de Sampling. Le fil rouge de l’épisode est le Amen Break , une boucle de batterie la plus samplée dans l’histoire de la musique. A travers l’écoute de nombreux extraits, découvrez à quel point le Amen Break est la source de courants musicaux comme le Hip Hop et la Jungle , ainsi que d’innombrables morceaux de tous styles des années 80 à aujourd’hui. En analysant de plus près cette boucle rythmique, cherchons à comprendre pourquoi elle est devenue la plus populaire d’entre toute. La partie 2 dans le prochain épisode fera la démonstration pratique dans Ableton Live de la technique du Breakbeat à partir du Amen Break original. Pour écouter cet épisode, lance tout simplement le lecteur ci-dessus. Tu peux aussi accéder à la page du podcast . Contenu de l'épisode #3: Savez-vous quel est le point commun entre ça: NWA – Straight Outta Compton Et ça: Terrorist – Renegade La réponse est le Amen Break ! Mais d’où cela vient-il? A la fin des années 60, un groupe nommé The Winstons a enregistré une chanson appelée « Amen Brother » . Au milieu du morceau, à environ 1’26’’, le batteur joue seul. Il ne se doute pas à ce moment-là qu’il allait influencer une bonne partie de la création musicale à venir. 1 - Le Breakbeat en tant que technique de sampling: Origines et évolutions Dans cet épisode, on va s’intéresser à une technique courante du beatmaking et plus que jamais pratiquée aujourd’hui dans tout style de musique rythmique, à savoir le « breakbeat » en tant que technique de sampling . Mais qu’est-ce qu’on appelle un « breakbeat » exactement? Pour le dire simplement, c’est un extrait rythmique d’un morceau de musique, où la batterie ou autre élément de percussion jouent en solo . Le terme «break» fait référence à ces sections de batterie jouées pendant le break du morceau; à savoir ces pauses dans l’arrangement mettant en valeur la rythmique seule. La technique du « breakbeat » est donc de s’approprier un extrait rythmique en l’extrayant d’anciens morceaux existants. En général, les morceaux en question sont de vieux enregistrements funk, soul, jazz ou R &B . La plupart sont centrés sur des arrangements vocaux et une instrumentation variée. Cependant, de nombreux morceaux comportent des segments de quatre ou huit mesures, souvent trouvés dans la transition entre couplet et refrain, dans lequel la batterie joue un certain groove en solo. Les éléments de la batterie (drums) Souvent, dans l’enregistrement récupéré sont extraits les éléments typiques de la batterie: Le Kick (grosse caisse), Snare (caisse claire), Clap, HiHat (charleys), Tom, Cymbale , etc… ainsi que des enchaînement particuliers de ceux-ci. Mais aussi d’autres percussions comme le Rim Shot, Bongo, Conga , ou autre tambourin . Ou encore d’autres éléments musicaux qui peuvent s’intégrer dans la rythmique comme: sample de Voix one shot, un riff de guitare, un jeu de cuivres Les origines du Hip Hop Alors cette technique du sampling ne date pas d’hier. Pour les producteurs de hip-hop et les DJ entreprenants des années 70, la découverte de ces breaks sur des disques vinyles a été l’occasion de réutiliser des rythmes existants à leurs propres fins musicales. C’était un moyen pour les jeunes musiciens à court d’argent de travailler avec de grands batteurs sans avoir besoin de trouver des musiciens de session ou de payer des frais de studio. Ce simple acte d’ingéniosité musicale a changé le cours de l’histoire de la musique. Pensez à l’âge d’or de la musique hip hop du milieu à la fin des années 80 et au début des années 90. Toute cette période de 10, 12 ans est principalement une période au cours de laquelle la musique hip hop, en particulier, récupère ces vieux échantillons de batterie. Les sampleurs sont devenus populaires à peu près au même moment où les musiciens ont commencé à utiliser des boîtes à rythmes et des synthétiseurs . Au début, c’était une sorte de nouveauté. Le sampling permettait à nouveau de produire des sons plus organiques contrairement au type de sons artificiels synthétisés. Les premières musiques électro, les premières musiques de breakdance, avaient un son très robotique, très futuriste. Y introduire le sampling, c’était en quelque sorte retrouver l’esthétique d’une période passée. Contrairement à aujourd’hui où on pratique essentiellement dans un logiciel dédié, les samplers à l’époque étaient uniquement des machines bien physiques, les fameuses MPC d’AKAI notamment, de la taille d’un lecteur DVD à peu près. Avec leur 16 Pads jouables avec les doigts (ce qu’on appelle du « finger drumming » ), ces samplers ont permis le découpage et ré-assemblage des breakbeats de façon très simple et rapide. Il y a aussi un côté nostalgique qui rend le sampling désirable. Lorsque les producteurs de l’époque mettent la main sur des sampleurs, ils réalisent qu’ils peuvent commencer à emprunter les sons des disques qu’ils ont écoutés en grandissant. Des compilations regroupant des vieux morceaux commençaient à apparaître. Des morceaux rigoureusement sélectionnés pour en retirer des breaks, des échantillons sonores à réutiliser. Dont le Amen Break en faisait évidemment parti. 2 - Exemples de reprise du « Amen Break » et autres breaks connus On en arrive donc à l’objet de cet épisode: le légendaire Amen Break . A la fin des années 60, un groupe nommé The Winstons a enregistré une chanson appelée « Amen Brother ». Au milieu du morceau, à environ 1’26’’, le batteur joue seul. Ce que vous venez d’entendre s’appelle depuis le « Amen Break », qui est donc ce solo de batterie jouée par GC Coleman le batteur du groupe. Alors pourquoi j’en parle? Car c’est tout simplement la boucle audio de batterie de loin la plus réutilisée de l’histoire de la musique . Ces quatre mesures de batterie jouées pendant sept secondes auront suffi à inspirer des milliers de futurs producteurs pour la reprendre et la remodeler à leur sauce dans leurs morceaux. Vous l’avez certainement entendu un million de fois, mais vous aurez peut-être du mal à vous souvenir dans quoi. Alors, écoutons à nouveau ces six secondes. Cette fois, voyez si vous pouvez vous rappeler où vous l’avez entendu. [Amen break à vitesse normale – 138 BPM] Amen break au ralenti: les débuts du Hip Hop On peut le retrouver joué au ralenti dans les débuts du Hip Hop : [I Desire – Salt-N-Pepa ] [Feel Alright Y’all – 2 Live Crew] [King of Beats – Mantronix ] Plus d’une décennie s’est écoulée depuis la sortie du morceau « Amen Brother » avant que le break ne commence à apparaître dans ces morceaux de hip hop. C’est principalement parce que le sampling ne devient une pratique à la mode qu’à partir des années 80. Puis dans du Hip Hop plus récent : [Streets on Fire – Lupe Fiasco] [Straight Outta Compton (radio edit)] [Red Eye – Big K.R.I.T.] [Compton – The Game feat. Will.i.am] [Pigs – Tyler, The Creator ] [Mindfields – Prodigy] Le Amen Break en accéléré dans la Jungle et Drum'n'Bass En ralentissant le Amen Break, il était très employé dans le hip hop, puis arriva la déferlante Jungle et la drum and bass avec le même break mais cette fois en accéléré. On ne compte plus le nombre de tracks de ce style crées à partir de ce break dans les années 90: [ Renegade – Terrorist ] [Tundra – Squarepusher] [ Can’t Knock The Hustle (Desired State Remix) – Jay-Z feat. Mary J Blige ] [ Ain’t Talkin’ Bout Dub – Apollo 440 ] La longueur et la polyvalence de l’Amen Break l’ont rendu si prolifique que trouver de nouvelles façons de l’utiliser était devenu une quête intellectuelle. [Fear – Amen Andrews (Luke Vibert)] [Creatures – Amon Tobin] [Nightlife – Amon Tobin] Polyvalence et diversité du Amen Break Ecoutons-le enfin dans des tracks plus récentes et dans des styles musicaux aussi divers qu’éclectiques: [ In for the Kill (Skream’s Let’s Get Ravey Remix) – La Roux ] [ Eyeless – Slipknot ] [Scary Monsters and Nice Sprites (The juggernaut remix) – Skrillex] [One Kiss (Oliver Helgens remix) – Calvin Harris and Dua Lipa] C’est même utilisé dans les pubs ou génériques comme ici avec le thème de Futurama: [Futurama Theme – Christopher Tyng ] Si vous allez sur l’application WhoSampled qui recense tous les samples de tous les morceaux du monde, on peut voir que plus de 5800 morceaux utilisent cette boucle Amen Break. Ce qui est absolument énorme! D’ailleurs je vous recommande cette application WhoSampled qui permet de vous rendre compte à quel point la musique, quelque soit le genre et l’origine, emprunte quasi toujours à ce qui a déjà existé dans le passé. L’ironie de l’histoire c’est que le batteur du Amen Break, GC Coleman ne touchera jamais un seul centime de son vivant de tout ce sampling. Autres breaks intéressants Bien sûr il y a des milliers d’autres breaks récupérés dans des vieux disques, tout comme le Amen break. Mais contrairement à aujourd’hui, où on peut simplement taper les mots « drum solo » dans un moteur de recherche et trouver une liste interminable d’enregistrements et de vidéos, il faut savoir que dans les années 80 et 90, les nombreux producteurs s’appuyaient sur la même série de collections de breaks disponibles dans le commerce ou des recommandations de bouche à oreille. A part le Amen Break, en voici quelques-uns devenus cultes: Incredible Bongo Band - Apache Ces premières mesures sont du morceau nommé Apache du Incredible Bongo Band. Apache est un incontournable du hip-hop grâce au légendaire « Merry-Go-Round » de DJ Kool Herc créant une boucle indéfinie du Break. Grandmaster Flash, DJ Shadow, Switch and The Sugarhill Gang (même s’il s’agit d’une reprise) ont tous connu le succès avec ce qui est connu depuis comme le Apache Break . James Brown - Funky Drummer, Tighten Up, ... Comme autre grand pourvoyeur de breaks, on retrouve bien sûr James Brown. Le son du kit de batterie « Funky Drummer » est aussi incomparable que le groove joué, ce qui en fait un grand classique à travers tous les âges. Public Enemy l’a utilisé plusieurs fois (jusqu’à même le nommer dans Fight The Power), tandis qu’il a également fait des apparitions sur des morceaux de LL Cool J, Run-DMC et NWA. Skull Snaps - It’s a new day Allez pour finir, je citerai Skull Snaps – It’s a new day qui a été repris entre autres par Prodigy, et Rob Dougan dans la BO de Matrix: —> Prodigy – Poison —> Rob Dougan – Clubbed to death (The Matrix) 3 - Le Amen Break décortiqué: pourquoi a t’il été si souvent utilisé? Maintenant voyons plus en détail pourquoi le Amen Break est devenu le breakbeat le plus populaire d’entre tous? Qu’est-ce qui fait que les artistes de tout genre musical l’utilisent tant? 1) La longueur du Amen break La première explication est la longueur de ce break. C’est en effet un échantillon de six-sept secondes, donc il y a beaucoup de matière à piocher dedans. Alors, bien sûr vous vous dites, six secondes cela semble peu. Mais il faut savoir que dans la pratique du sampling à ses débuts, six secondes, cela représentait une tonne de temps. Quand les gens fouillent dans les caisses de disques vinyles des magasins de disques d’occasion à la recherche du sample idéal, et qu’ils arrivent à trouver une seule mesure d’un échantillon de batterie, c’est une victoire. C’est pourquoi le Amen Break représente le Graal! 2) La variations des coups de batterie La deuxième explication est qu’en plus de sa longueur, le Amen Break est très varié dans ses coups de batterie. Le batteur , GC Coleman, fait son truc pendant 5 ou 6 secondes. Et il le fait suivant un certain groove bien à lui. Ce n’est pas suivant un rythme standard 4/4 four to the floor comme celui-ci: Mais au contraire on y entend un rythme syncopé où des temps normalement plus faibles sont mis en avant, notamment par les coups de la caisse claire . Cela a pour effet de disrupter le rythme naturel et créer une certaine tension rythmique, une vibe plus funky. 3) Les coups de la caisse claire dans la troisième mesure Par exemple, tous les coups de la caisse claire sont différents. La clé du succès rythmique est le décalage de cette caisse claire dans la troisième mesure. Coleman faisant presque allusion à la façon dont les producteurs de DnB pourraient l’utiliser plus tard. On peut alors découper le Amen break et réorganiser les divers coups individuels dans d’autres configurations. On peut très vite se lancer dans des motifs et des textures vraiment intéressants. Les transformations possibles du Amen Break En plus de réarranger le break, on peut l’accélérer… Le ralentir… ou même le jouer à l’envers (ce qu’on appelle faire un Reverse). D’autres effets comme le filtrage de fréquences, le changements de pitch, la distorsion, le stretching, etc… sont encore autant de traitements pour transformer le break en quelque chose de nouveau et d’unique. A bientôt pour la deuxième partie de cet épisode, où je ferai une démonstration pratique de transformation du Amen Break dans Ableton Live. Outro de l'épisode\n",
      "article_keywords: break, breakbeat, hip hop, jungle, loop, sampling\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54c74a-78fa-4684-b19b-f6bba6273cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a1d40-a113-4737-99ec-ac7e1c6114d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model='gpt-4o-mini'):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    #print(prompt)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58469b47-a3ee-4941-9466-d944c82d7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'De quel matériel ai-je besoin pour mon home studio?'\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be4d02-5d0f-465b-9b75-26dac1b1cbb7",
   "metadata": {},
   "source": [
    "# Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286045b2-a6a9-4120-9d75-8bdf4318f18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19281969-db02-42ae-a1a9-59c3caf86798",
   "metadata": {},
   "source": [
    "# RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c6999-12d3-41f2-ade9-9841722e5be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
